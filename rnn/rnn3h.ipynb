{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "id": "initial_id"
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f20cbbf1371df24",
   "metadata": {
    "id": "9f20cbbf1371df24"
   },
   "source": "### Setup and Configuration"
  },
  {
   "cell_type": "code",
   "id": "6635f43d7bbb2a6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6635f43d7bbb2a6e",
    "outputId": "e75120ea-b4a8-4983-9524-2714b5904a86"
   },
   "source": [
    "# Parameters for 3-Hour Prediction\n",
    "windowsize = 24      # Use the past 24 hours of data\n",
    "prediction_step = 3  # Predict the return 3 hours into the future\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "random.seed(1234)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7313e66f6691863",
   "metadata": {
    "id": "d7313e66f6691863"
   },
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "24a031ea2901ca54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24a031ea2901ca54",
    "outputId": "a531c528-427a-4f7b-9b2b-b02e25b102e3"
   },
   "source": [
    "print(\"Loading and resampling data...\")\n",
    "df_full = pd.read_csv('../data/btcusd_1-min_data.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_full['timestamp'] = pd.to_datetime(df_full['Timestamp'], unit='s')\n",
    "df_full = df_full.set_index('timestamp')\n",
    "\n",
    "print(\"Full dataset date range:\")\n",
    "print(f\"Start: {df_full.index.min()}\")\n",
    "print(f\"End: {df_full.index.max()}\")\n",
    "\n",
    "# Resample to hourly frequency instead of daily\n",
    "# approximate hourly close with last()\n",
    "df_hourly = df_full.loc['2015-01-01':]['Close'].resample('h').last().dropna().to_frame()\n",
    "\n",
    "print(f\"Resampled to {len(df_hourly)} hourly data points.\")\n",
    "\n",
    "print(df_hourly.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42c293201ae31fca",
   "metadata": {
    "id": "42c293201ae31fca"
   },
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "972ffcbb948d3b49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "972ffcbb948d3b49",
    "outputId": "5a18b474-e542-4985-f588-975e25772ec9"
   },
   "source": [
    "# Calculate hourly log returns\n",
    "df_hourly['log_return'] = np.log(df_hourly['Close'] / df_hourly['Close'].shift(1))\n",
    "# Calculate Moving Averages on 'Close' price\n",
    "df_hourly['SMA_5'] = df_hourly['Close'].rolling(window=5).mean()\n",
    "df_hourly['SMA_10'] = df_hourly['Close'].rolling(window=10).mean()\n",
    "\n",
    "# Calculate Volatility (rolling standard deviation of log returns)\n",
    "# We use log_return for volatility calculation, so it should be computed after log_return\n",
    "df_hourly['Volatility_10'] = df_hourly['log_return'].rolling(window=10).std()\n",
    "df_hourly['Volatility_20'] = df_hourly['log_return'].rolling(window=20).std()\n",
    "\n",
    "df_hourly = df_hourly.dropna().reset_index()\n",
    "\n",
    "print(f\"Calculated hourly log returns. Shape: {df_hourly.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9ca532954e79f47",
   "metadata": {
    "id": "d9ca532954e79f47"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "8eb7e2cc6610141b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eb7e2cc6610141b",
    "outputId": "5e66d577-e168-4846-9677-233cb54431d6"
   },
   "source": [
    "# -- Data Splitting (using hourly data) --\n",
    "split_fraction = 0.8\n",
    "split_idx = int(len(df_hourly) * split_fraction)\n",
    "train_data_raw = df_hourly.iloc[:split_idx].copy()\n",
    "test_data_raw = df_hourly.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Raw hourly training data shape: {train_data_raw.shape}\")\n",
    "print(f\"Raw hourly test data shape: {test_data_raw.shape}\")\n",
    "\n",
    "print(\"Test set date range:\")\n",
    "print(f\"Start: {test_data_raw.index.min()}\")\n",
    "print(f\"End: {test_data_raw.index.max()}\")\n",
    "\n",
    "# -- Normalization --\n",
    "feature_names_to_scale = ['log_return', 'SMA_5', 'SMA_10', 'Volatility_10', 'Volatility_20']\n",
    "scaled_feature_names = [f'scaled_{col}' for col in feature_names_to_scale]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data's features\n",
    "scaler.fit(train_data_raw[feature_names_to_scale])\n",
    "\n",
    "# Apply scaler to both train and test data and create new columns\n",
    "train_data_raw[scaled_feature_names] = scaler.transform(train_data_raw[feature_names_to_scale])\n",
    "test_data_raw[scaled_feature_names] = scaler.transform(test_data_raw[feature_names_to_scale])\n",
    "\n",
    "def create_lagged_data(data, windowsize, prediction_step, feature_cols_list, target_col='log_return', device=None):\n",
    "    \"\"\"\n",
    "    Create lagged data for predicting a specific future step (now hourly) with multiple features.\n",
    "    Args:\n",
    "        data: DataFrame with feature_cols and target_col (hourly data)\n",
    "        windowsize: Number of past hours for input features\n",
    "        prediction_step: How many hours ahead to predict (3 for 3-hour prediction)\n",
    "        feature_cols_list: List of column names for input features (e.g., ['scaled_log_return', 'scaled_SMA_5'])\n",
    "        target_col: Column name for target variable (e.g., 'log_return', unscaled)\n",
    "        device: Device to place tensors on\n",
    "    Returns:\n",
    "        x, y: PyTorch tensors of inputs (scaled) and targets (unscaled)\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    # Loop stops early enough for windowsize and prediction_step\n",
    "    for i in range(len(data) - windowsize - prediction_step + 1):\n",
    "        # Input features: PAST 'windowsize' scaled hourly features\n",
    "        # .values converts the DataFrame slice to a NumPy array of shape (windowsize, num_features)\n",
    "        feature_slice = data[feature_cols_list].iloc[i : i + windowsize].values\n",
    "\n",
    "        # Target: SUM OF ACTUAL (unscaled) returns for the next 'prediction_step' hours\n",
    "        target_indices = range(i + windowsize, i + windowsize + prediction_step)\n",
    "        target = data[target_col].iloc[target_indices].sum()  # Sum of next 'prediction_step' hourly returns\n",
    "\n",
    "        x.append(feature_slice)\n",
    "        y.append(target)\n",
    "\n",
    "    x = np.array(x) # Shape: (num_samples, windowsize, num_features)\n",
    "    y = np.array(y) # Shape: (num_samples,)\n",
    "\n",
    "    x_tensor = torch.FloatTensor(x).to(device)\n",
    "    y_tensor = torch.FloatTensor(y).to(device)\n",
    "\n",
    "    return x_tensor, y_tensor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1810eaaeb1d4201e",
   "metadata": {
    "id": "1810eaaeb1d4201e"
   },
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "15bcc48389cceb89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15bcc48389cceb89",
    "outputId": "ed61ff85-1544-4438-de41-91ab8aa407d0"
   },
   "source": [
    "# -- Create Datasets --\n",
    "x_train, y_train = create_lagged_data(\n",
    "    train_data_raw, windowsize, prediction_step,\n",
    "    feature_cols_list=scaled_feature_names,\n",
    "    target_col='log_return',\n",
    "    device=device\n",
    ")\n",
    "x_test, y_test = create_lagged_data(\n",
    "    test_data_raw, windowsize, prediction_step,\n",
    "    feature_cols_list=scaled_feature_names,\n",
    "    target_col='log_return',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Training input shape: {x_train.shape}, target shape: {y_train.shape}\")\n",
    "print(f\"Test input shape: {x_test.shape}, target shape: {y_test.shape}\")\n",
    "\n",
    "# Check if datasets are empty\n",
    "if x_train.shape[0] == 0 or x_test.shape[0] == 0:\n",
    "    raise ValueError(\"Created datasets are empty. Check data length after feature engineering, NaN dropping, and window/prediction steps.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc847e8c5a9cce41",
   "metadata": {
    "id": "bc847e8c5a9cce41"
   },
   "source": [
    "### Data Loader and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba48062d51a4c137",
   "metadata": {
    "id": "ba48062d51a4c137"
   },
   "source": [
    "# -- DataLoader --\n",
    "batch_size = 64\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# -- Model Definition --\n",
    "class RNN_model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, 30)\n",
    "        self.fc2 = torch.nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :] # Shape: (batch, hidden_size)\n",
    "        out = torch.nn.functional.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze(-1) # Shape: (batch)\n",
    "\n",
    "\n",
    "# -- Model, Optimizer, Loss (Adjusted Hyperparameters) --\n",
    "input_size = len(scaled_feature_names)\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "\n",
    "model = RNN_model(input_size, hidden_size, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.L1Loss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dff3c39eb4e0993e",
   "metadata": {
    "id": "dff3c39eb4e0993e"
   },
   "source": [
    "### Training Definition"
   ]
  },
  {
   "cell_type": "code",
   "id": "93652980a2ca41d2",
   "metadata": {
    "id": "93652980a2ca41d2"
   },
   "source": [
    "# -- Training Loop Function --\n",
    "def train_RNN(model, n_epochs, loader, optimizer, loss_fn, x_train, x_test, y_train, y_test, device):\n",
    "    train_losses = []\n",
    "    test_rmses = []\n",
    "    eval_batch_size = 128\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Evaluate on Training Set\n",
    "                train_preds_list = []\n",
    "                # Handle potential smaller train set in eval loop\n",
    "                current_pos = 0\n",
    "                while current_pos < len(x_train):\n",
    "                    x_batch_eval = x_train[current_pos:min(current_pos + eval_batch_size, len(x_train))]\n",
    "                    if x_batch_eval.shape[0] > 0: # Ensure batch not empty\n",
    "                         batch_preds = model(x_batch_eval)\n",
    "                         train_preds_list.append(batch_preds)\n",
    "                    current_pos += eval_batch_size\n",
    "                if not train_preds_list: # Handle case where train set is smaller than eval_batch_size\n",
    "                     train_rmse = float('nan')\n",
    "                else:\n",
    "                     y_pred_train = torch.cat(train_preds_list)\n",
    "                     train_rmse = torch.sqrt(loss_fn(y_pred_train, y_train))\n",
    "\n",
    "                # Evaluate on Test Set\n",
    "                test_preds_list = []\n",
    "                current_pos = 0\n",
    "                while current_pos < len(x_test):\n",
    "                     x_batch_eval = x_test[current_pos:min(current_pos + eval_batch_size, len(x_test))]\n",
    "                     if x_batch_eval.shape[0] > 0: # Ensure batch not empty\n",
    "                         batch_preds = model(x_batch_eval)\n",
    "                         test_preds_list.append(batch_preds)\n",
    "                     current_pos += eval_batch_size\n",
    "                if not test_preds_list: # Handle case where test set is smaller than eval_batch_size\n",
    "                    test_rmse = float('nan')\n",
    "                    test_rmses.append(test_rmse)\n",
    "                else:\n",
    "                    y_pred_test = torch.cat(test_preds_list)\n",
    "                    test_rmse = torch.sqrt(loss_fn(y_pred_test, y_test))\n",
    "                    test_rmses.append(test_rmse.item())\n",
    "\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train Loss {epoch_loss:.6f}, Train RMSE {train_rmse:.6f}, Test RMSE {test_rmse:.6f}\")\n",
    "    print(\"Training Finished.\")\n",
    "    return train_losses, test_rmses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "551b46a9952b8014",
   "metadata": {
    "id": "551b46a9952b8014"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b4f23ae8ce938",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b4f23ae8ce938",
    "outputId": "bd6c598b-cf42-48df-b19e-d76683d3eaf2"
   },
   "source": [
    "# -- Train the Model --\n",
    "train_losses, test_rmses = train_RNN(\n",
    "    model, n_epochs, train_loader, optimizer, loss_fn,\n",
    "    x_train, x_test, y_train, y_test, device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bee59d7a5480888",
   "metadata": {
    "id": "bee59d7a5480888"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "41577fcc6a17afa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41577fcc6a17afa4",
    "outputId": "c91529ed-e0d8-4c7c-d193-8d6337b1fb16"
   },
   "source": [
    "# -- Evaluation --\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test_final = model(x_test).cpu().numpy()\n",
    "\n",
    "y_actual_test = y_test.cpu().numpy()\n",
    "\n",
    "# Calculate performance metrics\n",
    "test_mse = mean_squared_error(y_actual_test, y_pred_test_final)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_actual_test, y_pred_test_final)\n",
    "mae = np.mean(np.abs(y_actual_test - y_pred_test_final))\n",
    "\n",
    "print(f\"Full Test Set Performance Metrics:\")\n",
    "print(f\"MSE: {test_mse:.8f}\")\n",
    "print(f\"RMSE: {test_rmse:.8f}\")\n",
    "print(f\"MAE: {mae:.8f}\")\n",
    "print(f\"R²: {test_r2:.6f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "586149f77f337393",
   "metadata": {
    "id": "586149f77f337393"
   },
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "code",
   "id": "8002b1e26a97f8ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8002b1e26a97f8ac",
    "outputId": "86576e72-9ae2-4567-a9e3-34eb91cf7cb7"
   },
   "source": [
    "# -- Simple Trading Strategy (3-Hour) --\n",
    "def generate_trading_signals(predictions, threshold=0):\n",
    "    signals = np.zeros(len(predictions))\n",
    "    signals[predictions > threshold] = 1\n",
    "    signals[predictions < -threshold] = -1\n",
    "    return signals\n",
    "\n",
    "# Generate signals\n",
    "signals = generate_trading_signals(y_pred_test_final, threshold=0) # Trade on any predicted direction\n",
    "print(f\"Trading signals generated. Shape: {signals.shape}\")\n",
    "print(f\"Signal distribution: Long: {np.sum(signals > 0)}, Short: {np.sum(signals < 0)}, Hold: {np.sum(signals == 0)}\")\n",
    "\n",
    "# Calculate strategy returns (signal * actual next 3-hour return)\n",
    "strategy_returns = signals * y_actual_test\n",
    "print(f\"Strategy returns calculated. Shape: {strategy_returns.shape}\")\n",
    "\n",
    "# Calculate cumulative returns\n",
    "cumulative_strategy_returns = np.cumsum(strategy_returns)\n",
    "cumulative_benchmark_returns = np.cumsum(y_actual_test)\n",
    "\n",
    "# Calculate Sharpe ratio (annualized for hourly data)\n",
    "if np.std(strategy_returns) > 1e-9:\n",
    "    # Annualization factor for hourly trading (approx 24*252 = 6048 trading hours per year)\n",
    "    trading_hours_per_year = 24 * 252\n",
    "    sharpe_ratio = (np.mean(strategy_returns) / np.std(strategy_returns)) * np.sqrt(trading_hours_per_year)\n",
    "else:\n",
    "    sharpe_ratio = 0.0\n",
    "    print(\"Warning: Standard deviation of strategy returns is zero or near-zero.\")\n",
    "\n",
    "print(f\"Strategy Sharpe Ratio (Annualized): {sharpe_ratio:.4f}\")\n",
    "\n",
    "print(signals)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "test_data_raw.iloc[windowsize + prediction_step - 1 : windowsize + prediction_step -1 + len(cumulative_strategy_returns)]['timestamp'].to_numpy()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNuJfgnrdLLn",
    "outputId": "eb8f6af9-f77d-462e-c3c8-543856f396ab"
   },
   "id": "XNuJfgnrdLLn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a2429e5bcd27b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "6a2429e5bcd27b2",
    "outputId": "a1ed02df-466b-4bec-e0c1-c180f95e3d9a"
   },
   "source": [
    "# -- Plotting (3-Hour) --\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Use test data timestamps for x-axis if available and aligned\n",
    "test_dates = test_data_raw.iloc[windowsize + prediction_step - 1 : windowsize + prediction_step -1 + len(cumulative_strategy_returns)]['timestamp'].to_numpy()\n",
    "\n",
    "if len(test_dates) == len(cumulative_strategy_returns):\n",
    "    plt.plot(test_dates, cumulative_strategy_returns, label=f'RNN Strategy (Predict 3 hours ahead)', color='cyan')\n",
    "    plt.plot(test_dates, cumulative_benchmark_returns, label='Buy & Hold (Benchmark)', color='orange')\n",
    "    plt.xlabel('Date')\n",
    "else:\n",
    "    # Fallback to plotting against index if dates don't align perfectly\n",
    "    print(\"Warning: Test dates length mismatch, plotting against index.\")\n",
    "    plt.plot(cumulative_strategy_returns, label=f'RNN Strategy (Predict 3 hours ahead)', color='cyan')\n",
    "    plt.plot(cumulative_benchmark_returns, label='Buy & Hold (Benchmark)', color='orange')\n",
    "    plt.xlabel('Trading Hours (Test Set)')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Cumulative Returns (3-Hour): RNN Strategy vs Buy & Hold (Window={windowsize} hours)')\n",
    "plt.ylabel('Cumulative Log Returns')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "253918b43e900c97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "253918b43e900c97",
    "outputId": "eb44c8e1-96ce-43c4-fcad-5acb52ddba46"
   },
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs (Hourly Data)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a152750196cadfc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "a152750196cadfc4",
    "outputId": "9af18619-f682-49e5-c358-d6b609ba9d59"
   },
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_actual_test, label='Actual Returns', color='blue', alpha=0.7)\n",
    "plt.plot(y_pred_test_final, label='Predicted Returns', color='red', alpha=0.7)\n",
    "plt.xlabel('Test Sample Index')\n",
    "plt.ylabel('3-Hour Log Return')\n",
    "plt.title('Predicted vs Actual Returns on Full Test Set')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot prediction error\n",
    "plt.figure(figsize=(14, 7))\n",
    "prediction_error = y_actual_test - y_pred_test_final\n",
    "plt.plot(prediction_error, color='green', alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Test Sample Index')\n",
    "plt.ylabel('Error (Actual - Predicted)')\n",
    "plt.title('Prediction Error on Full Test Set')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
