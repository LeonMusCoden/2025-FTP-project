{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup and Configuration",
   "id": "9f20cbbf1371df24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters for 3-Hour Prediction\n",
    "windowsize = 24      # Use the past 24 hours of data\n",
    "prediction_step = 3  # Predict the return 3 hours into the future\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "random.seed(1234)"
   ],
   "id": "6635f43d7bbb2a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading and Preprocessing",
   "id": "d7313e66f6691863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Loading and resampling data...\")\n",
    "df_full = pd.read_csv('../data/btcusd_1-min_data.csv')\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_full['timestamp'] = pd.to_datetime(df_full['Timestamp'], unit='s')\n",
    "df_full = df_full.set_index('timestamp')\n",
    "\n",
    "# Resample to hourly frequency instead of daily\n",
    "# approximate hourly close with last()\n",
    "df_hourly = df_full['Close'].resample('h').last().to_frame()\n",
    "\n",
    "# Drop hours with no data\n",
    "df_hourly = df_hourly.dropna()\n",
    "\n",
    "print(f\"Resampled to {len(df_hourly)} hourly data points.\")"
   ],
   "id": "24a031ea2901ca54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features",
   "id": "42c293201ae31fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate hourly log returns\n",
    "df_hourly['log_return'] = np.log(df_hourly['Close'] / df_hourly['Close'].shift(1))\n",
    "df_hourly = df_hourly.dropna().reset_index()\n",
    "\n",
    "print(f\"Calculated hourly log returns. Shape: {df_hourly.shape}\")"
   ],
   "id": "972ffcbb948d3b49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation",
   "id": "d9ca532954e79f47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Data Splitting (using hourly data) --\n",
    "split_fraction = 0.8\n",
    "split_idx = int(len(df_hourly) * split_fraction)\n",
    "train_data_raw = df_hourly.iloc[:split_idx].copy()\n",
    "test_data_raw = df_hourly.iloc[split_idx:].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"Raw hourly training data shape: {train_data_raw.shape}\")\n",
    "print(f\"Raw hourly test data shape: {test_data_raw.shape}\")\n",
    "\n",
    "# -- Normalization (using hourly log returns) --\n",
    "scaler = StandardScaler()\n",
    "# Fit scaler ONLY on training data's hourly log_return\n",
    "scaler.fit(train_data_raw['log_return'].values.reshape(-1, 1))\n",
    "\n",
    "# Apply scaler to both train and test data\n",
    "train_data_raw['scaled_log_return'] = scaler.transform(train_data_raw['log_return'].values.reshape(-1, 1))\n",
    "test_data_raw['scaled_log_return'] = scaler.transform(test_data_raw['log_return'].values.reshape(-1, 1))\n",
    "\n",
    "# -- Create Lagged Data Function (Now works for hourly steps) --\n",
    "def create_lagged_data(data, windowsize, prediction_step, feature_col='scaled_log_return', target_col='log_return', device=None):\n",
    "    \"\"\"\n",
    "    Create lagged data for predicting a specific future step (now hourly).\n",
    "    Args:\n",
    "        data: DataFrame with feature_col and target_col (hourly data)\n",
    "        windowsize: Number of past hours for input features\n",
    "        prediction_step: How many hours ahead to predict (3 for 3-hour prediction)\n",
    "        feature_col: Column name for input features (e.g., 'scaled_log_return')\n",
    "        target_col: Column name for target variable (e.g., 'log_return')\n",
    "        device: Device to place tensors on\n",
    "    Returns:\n",
    "        x, y: PyTorch tensors of inputs (scaled) and targets (unscaled)\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    # Loop stops early enough for windowsize and prediction_step\n",
    "    for i in range(len(data) - windowsize - prediction_step + 1):\n",
    "        # Input features: PAST 'windowsize' scaled hourly returns\n",
    "        feature = data[feature_col].iloc[i : i + windowsize].values\n",
    "\n",
    "        # Target: SUM OF ACTUAL (unscaled) returns for the next 3 hours\n",
    "        # We're predicting the CUMULATIVE return over the next 3 hours\n",
    "        target_indices = range(i + windowsize, i + windowsize + prediction_step)\n",
    "        target = data[target_col].iloc[target_indices].sum()  # Sum of next 3 hourly returns\n",
    "\n",
    "        x.append(feature)\n",
    "        y.append(target)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    x_tensor = torch.FloatTensor(x).to(device)\n",
    "    y_tensor = torch.FloatTensor(y).to(device)\n",
    "\n",
    "    return x_tensor, y_tensor"
   ],
   "id": "8eb7e2cc6610141b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and Testing Datasets",
   "id": "1810eaaeb1d4201e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Create Datasets (using hourly data) --\n",
    "x_train, y_train = create_lagged_data(\n",
    "    train_data_raw, windowsize, prediction_step,\n",
    "    feature_col='scaled_log_return', target_col='log_return', device=device\n",
    ")\n",
    "x_test, y_test = create_lagged_data(\n",
    "    test_data_raw, windowsize, prediction_step,\n",
    "    feature_col='scaled_log_return', target_col='log_return', device=device\n",
    ")\n",
    "\n",
    "print(f\"Training input shape: {x_train.shape}, target shape: {y_train.shape}\")\n",
    "print(f\"Test input shape: {x_test.shape}, target shape: {y_test.shape}\")\n",
    "\n",
    "# Check if datasets are empty (can happen if not enough hourly data)\n",
    "if x_train.shape[0] == 0 or x_test.shape[0] == 0:\n",
    "    raise ValueError(\"Created datasets are empty. Check data length after resampling and window/prediction steps.\")"
   ],
   "id": "15bcc48389cceb89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loader and Model Definition",
   "id": "bc847e8c5a9cce41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- DataLoader --\n",
    "batch_size = 64  # Increased since hourly data will give us more samples\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# -- Model Definition --\n",
    "class RNN_model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, 30)\n",
    "        self.fc2 = torch.nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1) # Shape: (batch, seq_len=windowsize, input_size=1)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :] # Shape: (batch, hidden_size)\n",
    "        out = torch.nn.functional.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze(-1) # Shape: (batch)\n",
    "\n",
    "\n",
    "# -- Model, Optimizer, Loss (Adjusted Hyperparameters) --\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 30\n",
    "\n",
    "model = RNN_model(input_size, hidden_size, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.MSELoss()"
   ],
   "id": "ba48062d51a4c137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Definition",
   "id": "dff3c39eb4e0993e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Training Loop Function --\n",
    "def train_RNN(model, n_epochs, loader, optimizer, loss_fn, x_train, x_test, y_train, y_test, device):\n",
    "    train_losses = []\n",
    "    test_rmses = []\n",
    "    eval_batch_size = 128\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Evaluate on Training Set\n",
    "                train_preds_list = []\n",
    "                # Handle potential smaller train set in eval loop\n",
    "                current_pos = 0\n",
    "                while current_pos < len(x_train):\n",
    "                    x_batch_eval = x_train[current_pos:min(current_pos + eval_batch_size, len(x_train))]\n",
    "                    if x_batch_eval.shape[0] > 0: # Ensure batch not empty\n",
    "                         batch_preds = model(x_batch_eval)\n",
    "                         train_preds_list.append(batch_preds)\n",
    "                    current_pos += eval_batch_size\n",
    "                if not train_preds_list: # Handle case where train set is smaller than eval_batch_size\n",
    "                     train_rmse = float('nan')\n",
    "                else:\n",
    "                     y_pred_train = torch.cat(train_preds_list)\n",
    "                     train_rmse = torch.sqrt(loss_fn(y_pred_train, y_train))\n",
    "\n",
    "                # Evaluate on Test Set\n",
    "                test_preds_list = []\n",
    "                current_pos = 0\n",
    "                while current_pos < len(x_test):\n",
    "                     x_batch_eval = x_test[current_pos:min(current_pos + eval_batch_size, len(x_test))]\n",
    "                     if x_batch_eval.shape[0] > 0: # Ensure batch not empty\n",
    "                         batch_preds = model(x_batch_eval)\n",
    "                         test_preds_list.append(batch_preds)\n",
    "                     current_pos += eval_batch_size\n",
    "                if not test_preds_list: # Handle case where test set is smaller than eval_batch_size\n",
    "                    test_rmse = float('nan')\n",
    "                    test_rmses.append(test_rmse)\n",
    "                else:\n",
    "                    y_pred_test = torch.cat(test_preds_list)\n",
    "                    test_rmse = torch.sqrt(loss_fn(y_pred_test, y_test))\n",
    "                    test_rmses.append(test_rmse.item())\n",
    "\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train Loss {epoch_loss:.6f}, Train RMSE {train_rmse:.6f}, Test RMSE {test_rmse:.6f}\")\n",
    "    print(\"Training Finished.\")\n",
    "    return train_losses, test_rmses"
   ],
   "id": "93652980a2ca41d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "551b46a9952b8014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Train the Model --\n",
    "train_losses, test_rmses = train_RNN(\n",
    "    model, n_epochs, train_loader, optimizer, loss_fn,\n",
    "    x_train, x_test, y_train, y_test, device\n",
    ")"
   ],
   "id": "5b4f23ae8ce938",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "bee59d7a5480888"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Evaluation --\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test_final = model(x_test).cpu().numpy()\n",
    "\n",
    "y_actual_test = y_test.cpu().numpy()\n",
    "\n",
    "# Calculate final metrics\n",
    "test_mse = mean_squared_error(y_actual_test, y_pred_test_final)\n",
    "test_r2 = r2_score(y_actual_test, y_pred_test_final)\n",
    "print(f\"\\nFinal Test MSE: {test_mse:.8f}\")\n",
    "print(f\"Final Test R²: {test_r2:.6f}\")"
   ],
   "id": "41577fcc6a17afa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Strategy",
   "id": "586149f77f337393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Simple Trading Strategy (3-Hour) --\n",
    "def generate_trading_signals(predictions, threshold=0):\n",
    "    signals = np.zeros_like(predictions)\n",
    "    signals[predictions > threshold] = 1\n",
    "    signals[predictions < -threshold] = -1\n",
    "    return signals\n",
    "\n",
    "# Generate signals\n",
    "signals = generate_trading_signals(y_pred_test_final, threshold=0) # Trade on any predicted direction\n",
    "print(f\"Trading signals generated. Shape: {signals.shape}\")\n",
    "print(f\"Signal distribution: Long: {np.sum(signals > 0)}, Short: {np.sum(signals < 0)}, Hold: {np.sum(signals == 0)}\")\n",
    "\n",
    "# Calculate strategy returns (signal * actual next 3-hour return)\n",
    "strategy_returns = signals * y_actual_test\n",
    "print(f\"Strategy returns calculated. Shape: {strategy_returns.shape}\")\n",
    "\n",
    "# Calculate cumulative returns\n",
    "cumulative_strategy_returns = np.cumsum(strategy_returns)\n",
    "cumulative_benchmark_returns = np.cumsum(y_actual_test)\n",
    "\n",
    "# Calculate Sharpe ratio (annualized for hourly data)\n",
    "if np.std(strategy_returns) > 1e-9:\n",
    "    # Annualization factor for hourly trading (approx 24*252 = 6048 trading hours per year)\n",
    "    trading_hours_per_year = 24 * 252\n",
    "    sharpe_ratio = (np.mean(strategy_returns) / np.std(strategy_returns)) * np.sqrt(trading_hours_per_year)\n",
    "else:\n",
    "    sharpe_ratio = 0.0\n",
    "    print(\"Warning: Standard deviation of strategy returns is zero or near-zero.\")\n",
    "\n",
    "print(f\"Strategy Sharpe Ratio (Annualized): {sharpe_ratio:.4f}\")"
   ],
   "id": "8002b1e26a97f8ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Plotting (3-Hour) --\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Use test data timestamps for x-axis if available and aligned\n",
    "test_dates = test_data_raw.iloc[windowsize + prediction_step - 1 : windowsize + prediction_step -1 + len(cumulative_strategy_returns)]['timestamp']\n",
    "\n",
    "if len(test_dates) == len(cumulative_strategy_returns):\n",
    "    plt.plot(test_dates, cumulative_strategy_returns, label=f'RNN Strategy (Predict 3 hours ahead)', color='cyan')\n",
    "    plt.plot(test_dates, cumulative_benchmark_returns, label='Buy & Hold (Benchmark)', color='orange')\n",
    "    plt.xlabel('Date')\n",
    "else:\n",
    "    # Fallback to plotting against index if dates don't align perfectly\n",
    "    print(\"Warning: Test dates length mismatch, plotting against index.\")\n",
    "    plt.plot(cumulative_strategy_returns, label=f'RNN Strategy (Predict 3 hours ahead)', color='cyan')\n",
    "    plt.plot(cumulative_benchmark_returns, label='Buy & Hold (Benchmark)', color='orange')\n",
    "    plt.xlabel('Trading Hours (Test Set)')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Cumulative Returns (3-Hour): RNN Strategy vs Buy & Hold (Window={windowsize} hours)')\n",
    "plt.ylabel('Cumulative Log Returns')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "6a2429e5bcd27b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs (Hourly Data)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "253918b43e900c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a152750196cadfc4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
